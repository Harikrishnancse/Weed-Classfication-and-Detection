{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv3ZFzSW50ey",
        "outputId": "192e2369-9e53-4af2-c1f9-6c80e7d6223a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy==1.17.5 in /usr/local/lib/python3.7/dist-packages (1.17.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.17.5;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHpCEUUw6j9H",
        "outputId": "0bc395d1-6cf3-46f8-f5a9-e4590f2990f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Tensorflow 1 is deprecated, and support will be removed on August 1, 2022.\n",
            "After that, `%tensorflow_version 1.x` will throw an error.\n",
            "\n",
            "Your notebook should be updated to use Tensorflow 2.\n",
            "See the guide at https://www.tensorflow.org/guide/migrate#migrate-from-tensorflow-1x-to-tensorflow-2.\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf_slim==1.1.0\n",
            "  Using cached tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim==1.1.0) (1.1.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install tf_slim==1.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25bCkLgf6y7n"
      },
      "outputs": [],
      "source": [
        "# For faster training time, images should be resized to 300x300 and then annotated\n",
        "# Images should contain the objects of interest at various scales, angles, lighting conditions, locations\n",
        "# For acceptable results - mAP@0.5 of 0.9 the model was trained with batch size of 24\n",
        "# and 10000 steps. this takes about 1h using 2 augmentations. \n",
        "# using 5 augmentations it takes about 2h \n",
        "num_steps = 10000  # A step means using a single batch of data. larger batch, less steps required\n",
        "#Number of evaluation steps.\n",
        "num_eval_steps = 50\n",
        "#Batch size 24 is a setting that generally works well. can be changed higher or lower \n",
        "MODELS_CONFIG = {\n",
        "        'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "        'batch_size': 8\n",
        "    }\n",
        "}\n",
        "selected_model = 'ssd_mobilenet_v2'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "# Name of the pipline file in tensorflow object detection API.\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Training batch size fits in Colab's GPU memory for selected model.\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vnmPbZJ67rv",
        "outputId": "29bc7094-1176-49c3-8756-1f16571d0f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into '/content/depthai-ml-training'...\n",
            "remote: Enumerating objects: 493, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 493 (delta 4), reused 4 (delta 1), pack-reused 455\u001b[K\n",
            "Receiving objects: 100% (493/493), 100.97 MiB | 39.91 MiB/s, done.\n",
            "Resolving deltas: 100% (236/236), done.\n",
            "/content/depthai-ml-training\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "repo_url = 'https://github.com/luxonis/depthai-ml-training.git'\n",
        "import os\n",
        "%cd /content\n",
        "repo_dir_path = \"/content/depthai-ml-training\"\n",
        "!rm -rf {repo_dir_path}\n",
        "!git clone {repo_url} {repo_dir_path}\n",
        "%cd {repo_dir_path}\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wGD_ijj7A26",
        "outputId": "12ae4166-326f-462d-d3ce-ec69608c348c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#mount your google drive.\n",
        "#it will be visible in the file navigator on the left of this notebook\n",
        "#there should be a folder in your drive with your data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ulgNxeR7TAw"
      },
      "outputs": [],
      "source": [
        "#copy files from gdrive to colab drive. this takes a few minutes, depending on the number of files.\n",
        "#Go on the file explorer on the left of this notebook and access your gdrive. find the folders\n",
        "#with your train, test and final_test images.\n",
        "#Right click on each and copy the path. paste it btw the first \" \" in the corresponding lines  \n",
        "\n",
        "#training folder\n",
        "!cp -r \"/content/gdrive/MyDrive/New_Obj_det_Datset/train\" \"/content/train\"\n",
        "#validation folder\n",
        "!cp -r \"/content/gdrive/MyDrive/New_Obj_det_Datset/validation\" \"/content/validation\"\n",
        "#test folder\n",
        "!cp -r \"/content/gdrive/MyDrive/New_Obj_det_Datset/test\" \"/content/test\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_dBiTzF7yzW",
        "outputId": "d2a2a43a-e2e3-43d9-8bc9-3fcafcface85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'cot (2).jpg'  'cw (12).JPG'  'ec (2).JPG'   'mg (12).JPG'  'rw (10).jpg'\n",
            "'cot (2).xml'  'cw (12).xml'  'ec (2).xml'   'mg (12).xml'  'rw (10).xml'\n",
            "'cot (3).jpg'  'cw (2).JPG'   'ec (3).JPG'   'mg (1).jpg'   'rw (2).jpg'\n",
            "'cot (3).xml'  'cw (2).xml'   'ec (3).xml'   'mg (1).xml'   'rw (2).xml'\n",
            "'cot (4).jpg'  'cw (3).JPG'   'ec (4).JPG'   'mg (3).JPG'   'rw (3).jpg'\n",
            "'cot (4).xml'  'cw (3).xml'   'ec (4).xml'   'mg (3).xml'   'rw (3).xml'\n",
            "'cot (6).jpg'  'cw (6).JPG'   'ec (5).JPG'   'mg (4).JPG'   'rw (4).jpg'\n",
            "'cot (6).xml'  'cw (6).xml'   'ec (5).xml'   'mg (4).xml'   'rw (4).xml'\n",
            "'cot (8).jpg'  'cw (7).JPG'   'ec (6).JPG'   'mg (5).JPG'   'rw (5).jpg'\n",
            "'cot (8).xml'  'cw (7).xml'   'ec (6).xml'   'mg (5).xml'   'rw (5).xml'\n",
            "'cw (10).jpg'  'cw (8).jpg'   'ec (8).JPG'   'mg (6).JPG'   'rw (6).JPG'\n",
            "'cw (10).xml'  'cw (8).xml'   'ec (8).xml'   'mg (6).xml'   'rw (6).xml'\n",
            "'cw (11).jpg'  'ec (10).JPG'  'mg (10).JPG'  'mg (8).JPG'   'rw (8).jpg'\n",
            "'cw (11).xml'  'ec (10).xml'  'mg (10).xml'  'mg (8).xml'   'rw (8).xml'\n"
          ]
        }
      ],
      "source": [
        "# quick check for training data files. you can also browse to the object_detection_demo_flows\n",
        "# on the left and see if they were copied\n",
        "!ls /content/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTGwxHb_720V",
        "outputId": "8f554cd9-92a6-47c1-ba82-975dd4a64440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/models\n",
            "Note: checking out '58d19c67e1d30d905dd5c6e5092348658fed80af'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 58d19c67e Internal change\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [816 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,521 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,075 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,099 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,873 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,063 kB]\n",
            "Get:20 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.9 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,321 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,298 kB]\n",
            "Fetched 15.4 MB in 10s (1,612 kB/s)\n",
            "Reading package lists... Done\n",
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.6_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.6) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.7_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1ubuntu0.7) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.6) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1ubuntu0.7) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.4 MB/s \n",
            "\u001b[?25hFound existing installation: pycocotools 2.0.4\n",
            "Uninstalling pycocotools-2.0.4:\n",
            "  Successfully uninstalled pycocotools-2.0.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycocotools==2.0.4\n",
            "  Downloading pycocotools-2.0.4.tar.gz (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 5.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.4) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.4) (1.17.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools==2.0.4) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0.4) (1.15.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.4-cp37-cp37m-linux_x86_64.whl size=265219 sha256=f42ed0a30df5041397cabb89f0c4c0c3b5c1b07ca677088b3996b13388b84e48\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a08zgo1t/wheels/a3/5f/fa/f011e578cc76e1fc5be8dce30b3eb9fd00f337e744b3bba59b\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "Successfully installed pycocotools-2.0.4\n",
            "/content/models/research\n",
            "object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used.\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "%cd /content/models/\n",
        "!git checkout 58d19c67e1d30d905dd5c6e5092348658fed80af\n",
        "!apt-get update && apt-get install -y -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q Cython==0.29.28 contextlib2==0.5.5 pillow==7.1.2 lxml==4.2.6 matplotlib==3.2.2\n",
        "!pip uninstall -y pycocotools\n",
        "!pip install --no-cache-dir pycocotools==2.0.4\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "!python object_detection/builders/model_builder_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAMkVZhW8L6t",
        "outputId": "7ad9ab7e-fdb1-466d-fbf3-a9e662f76c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Successfully converted xml to csv.\n",
            "Generate `/content/label_map.pbtxt`\n",
            "Successfully converted xml to csv.\n",
            "WARNING:tensorflow:From depthai-ml-training/helpers/generate_tfrecord.py:138: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From depthai-ml-training/helpers/generate_tfrecord.py:111: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0712 18:50:33.017861 140647394457472 module_wrapper.py:139] From depthai-ml-training/helpers/generate_tfrecord.py:111: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From depthai-ml-training/helpers/generate_tfrecord.py:57: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0712 18:50:33.044625 140647394457472 module_wrapper.py:139] From depthai-ml-training/helpers/generate_tfrecord.py:57: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/train.record\n",
            "WARNING:tensorflow:From depthai-ml-training/helpers/generate_tfrecord.py:138: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From depthai-ml-training/helpers/generate_tfrecord.py:111: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0712 18:50:35.416149 140191951382400 module_wrapper.py:139] From depthai-ml-training/helpers/generate_tfrecord.py:111: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From depthai-ml-training/helpers/generate_tfrecord.py:57: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0712 18:50:35.425627 140191951382400 module_wrapper.py:139] From depthai-ml-training/helpers/generate_tfrecord.py:57: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/test.record\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "%cd /content/\n",
        "\n",
        "# Convert train folder annotation xml files to a single csv file,\n",
        "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
        "!python depthai-ml-training/helpers/xml_to_csv.py -i /content/train/ -o /content/train_labels.csv -l /content\n",
        "# Convert test folder annotation xml files to a single csv.\n",
        "!python depthai-ml-training/helpers/xml_to_csv.py -i /content/validation/ -o /content/test_labels.csv\n",
        "\n",
        "# Generate `train.record`\n",
        "!python depthai-ml-training/helpers/generate_tfrecord.py --csv_input=/content/train_labels.csv --output_path=/content/train.record --img_path=/content/train --label_map label_map.pbtxt\n",
        "\n",
        "# Generate `test.record`\n",
        "!python depthai-ml-training/helpers/generate_tfrecord.py --csv_input=/content/test_labels.csv --output_path=/content/test.record --img_path=/content/validation --label_map label_map.pbtxt\n",
        "\n",
        "# Set the paths\n",
        "test_record_fname = '/content/test.record'\n",
        "train_record_fname = '/content/train.record'\n",
        "label_map_pbtxt_fname = '/content/label_map.pbtxt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rupy_zh08anq",
        "outputId": "ee66b1c8-9db3-4de9-cdf2-4a029fa37a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n",
            "/content/models/research/pretrained_model\n",
            "total 135M\n",
            "drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 .\n",
            "drwxr-xr-x 63 root   root  4.0K Jul 12 18:50 ..\n",
            "-rw-r--r--  1 345018 89939   77 Mar 30  2018 checkpoint\n",
            "-rw-r--r--  1 345018 89939  67M Mar 30  2018 frozen_inference_graph.pb\n",
            "-rw-r--r--  1 345018 89939  65M Mar 30  2018 model.ckpt.data-00000-of-00001\n",
            "-rw-r--r--  1 345018 89939  15K Mar 30  2018 model.ckpt.index\n",
            "-rw-r--r--  1 345018 89939 3.4M Mar 30  2018 model.ckpt.meta\n",
            "-rw-r--r--  1 345018 89939 4.2K Mar 30  2018 pipeline.config\n",
            "drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 saved_model\n"
          ]
        }
      ],
      "source": [
        "%cd /content/models/research\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)\n",
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "L_LDXN3q8jAt",
        "outputId": "bdbe4c8d-e794-4a4a-81fd-1dc61923092f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/models/research/pretrained_model/model.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#TF pretrained model checkpoint\n",
        "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "fine_tune_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeoSSX0-8tEr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n",
        "\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssOzcBSh8w8k",
        "outputId": "68301e2b-4430-4c25-d4ba-c1883d720359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "depthai-ml-training  models\t  test_labels.csv  train_labels.csv\n",
            "gdrive\t\t     sample_data  test.record\t   train.record\n",
            "label_map.pbtxt      test\t  train\t\t   validation\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "parent = Path(label_map_pbtxt_fname).parent\n",
        "!ls {parent}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLxD2nbH80wz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "iou_threshold = 0.50\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open(pipeline_fname, 'w') as f:\n",
        "    \n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test.\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "\n",
        "    # label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('iou_threshold: [0-9].[0-9]+',\n",
        "               'iou_threshold: {}'.format(iou_threshold), s)\n",
        "    \n",
        "    f.write(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv2e6LBJ9DSw",
        "outputId": "31f4f1f8-8387-4dcf-d1f1-6208de5dc082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# SSD with Mobilenet v2 configuration for MSCOCO Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 5\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 1\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            truncated_normal_initializer {\n",
            "              stddev: 0.03\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          train: true,\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid {\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      hard_example_miner {\n",
            "        num_hard_examples: 3000\n",
            "        iou_threshold: 0.5\n",
            "        loss_type: CLASSIFICATION\n",
            "        max_negatives_per_positive: 3\n",
            "        min_negatives_per_image: 3\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.5\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 8\n",
            "  optimizer {\n",
            "    rms_prop_optimizer: {\n",
            "      learning_rate: {\n",
            "        exponential_decay_learning_rate {\n",
            "          initial_learning_rate: 0.004\n",
            "          decay_steps: 800720\n",
            "          decay_factor: 0.95\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "      decay: 0.9\n",
            "      epsilon: 1.0\n",
            "    }\n",
            "  }\n",
            "  fine_tune_checkpoint: \"/content/models/research/pretrained_model/model.ckpt\"\n",
            "  fine_tune_checkpoint_type:  \"detection\"\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 10000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/train.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  num_examples: 8000\n",
            "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
            "  # Remove the below line to evaluate indefinitely.\n",
            "  max_evals: 10\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/test.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}"
          ]
        }
      ],
      "source": [
        "# #Have a look at the config file with various settings\n",
        "!cat {pipeline_fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DmBOgsJ9Gcm",
        "outputId": "6006134d-6fed-4b04-ee15-e928b29bd6c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
            "W0712 18:50:43.680885 140178538243968 model_lib.py:717] Forced number of epochs for all eval validations to be 1.\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 10000\n",
            "I0712 18:50:43.681145 140178538243968 config_util.py:552] Maybe overwriting train_steps: 10000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0712 18:50:43.681226 140178538243968 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "I0712 18:50:43.681289 140178538243968 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
            "I0712 18:50:43.681351 140178538243968 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
            "INFO:tensorflow:Maybe overwriting load_pretrained: True\n",
            "I0712 18:50:43.681407 140178538243968 config_util.py:552] Maybe overwriting load_pretrained: True\n",
            "INFO:tensorflow:Ignoring config override key: load_pretrained\n",
            "I0712 18:50:43.681461 140178538243968 config_util.py:562] Ignoring config override key: load_pretrained\n",
            "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "W0712 18:50:43.681562 140178538243968 model_lib.py:733] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "I0712 18:50:43.681639 140178538243968 model_lib.py:768] create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7dc8e04c10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "I0712 18:50:43.682121 140178538243968 estimator.py:212] Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7dc8e04c10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f7dc8b31cb0>) includes params argument, but params are not passed to Estimator.\n",
            "W0712 18:50:43.682351 140178538243968 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f7dc8b31cb0>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "I0712 18:50:43.682788 140178538243968 estimator_training.py:186] Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "I0712 18:50:43.682953 140178538243968 training.py:612] Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "I0712 18:50:43.683145 140178538243968 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0712 18:50:43.697983 140178538243968 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0712 18:50:43.724249 140178538243968 dataset_builder.py:83] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0712 18:50:43.729491 140178538243968 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0712 18:50:43.752366 140178538243968 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7dd056d690>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0712 18:50:43.785053 140178538243968 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7dd056d690>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f7dc91be170> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0712 18:50:44.094396 140178538243968 ag_logging.py:146] Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f7dc91be170> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:79: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0712 18:50:44.100999 140178538243968 deprecation.py:323] From /content/models/research/object_detection/inputs.py:79: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0712 18:50:44.108612 140178538243968 deprecation.py:323] From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0712 18:50:44.203419 140178538243968 deprecation.py:323] From /content/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:260: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0712 18:50:44.978760 140178538243968 deprecation.py:323] From /content/models/research/object_detection/inputs.py:260: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0712 18:50:45.479186 140178538243968 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0712 18:50:45.612821 140178538243968 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 18:50:48.495335 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 18:50:48.530195 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 18:50:48.563706 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 18:50:48.597166 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 18:50:48.630087 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 18:50:48.662997 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "W0712 18:50:48.706119 140178538243968 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 512]], model variable shape: [[3, 3, 256, 512]]. This variable will not be initialized from the checkpoint.\n",
            "W0712 18:50:48.706355 140178538243968 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 256]], model variable shape: [[3, 3, 128, 256]]. This variable will not be initialized from the checkpoint.\n",
            "W0712 18:50:48.706468 140178538243968 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 256]], model variable shape: [[3, 3, 128, 256]]. This variable will not be initialized from the checkpoint.\n",
            "W0712 18:50:48.706579 140178538243968 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 64, 128]], model variable shape: [[3, 3, 64, 128]]. This variable will not be initialized from the checkpoint.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0712 18:50:52.828206 140178538243968 deprecation.py:506] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0712 18:50:59.222548 140178538243968 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0712 18:50:59.224068 140178538243968 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0712 18:51:02.565998 140178538243968 monitored_session.py:240] Graph was finalized.\n",
            "2022-07-12 18:51:02.581460: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2022-07-12 18:51:02.582963: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x852a700 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-07-12 18:51:02.583004: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-07-12 18:51:02.616503: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-07-12 18:51:02.684855: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2022-07-12 18:51:02.684916: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cdb5970f856a): /proc/driver/nvidia/version does not exist\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0712 18:51:06.976650 140178538243968 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0712 18:51:07.348833 140178538243968 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into training/model.ckpt.\n",
            "I0712 18:51:16.304877 140178538243968 basic_session_run_hooks.py:606] Saving checkpoints for 0 into training/model.ckpt.\n",
            "INFO:tensorflow:loss = 5.2379227, step = 1\n",
            "I0712 18:51:28.902166 140178538243968 basic_session_run_hooks.py:262] loss = 5.2379227, step = 1\n",
            "INFO:tensorflow:global_step/sec: 1.37019\n",
            "I0712 18:52:41.884267 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 1.37019\n",
            "INFO:tensorflow:loss = 2.674332, step = 101 (72.983 sec)\n",
            "I0712 18:52:41.885192 140178538243968 basic_session_run_hooks.py:260] loss = 2.674332, step = 101 (72.983 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.25321\n",
            "I0712 18:54:01.679429 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 1.25321\n",
            "INFO:tensorflow:loss = 3.3535142, step = 201 (79.796 sec)\n",
            "I0712 18:54:01.680937 140178538243968 basic_session_run_hooks.py:260] loss = 3.3535142, step = 201 (79.796 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.990528\n",
            "I0712 18:55:42.635761 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.990528\n",
            "INFO:tensorflow:loss = 2.0025907, step = 301 (100.956 sec)\n",
            "I0712 18:55:42.637075 140178538243968 basic_session_run_hooks.py:260] loss = 2.0025907, step = 301 (100.956 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.98925\n",
            "I0712 18:57:23.722426 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.98925\n",
            "INFO:tensorflow:loss = 1.8321568, step = 401 (101.087 sec)\n",
            "I0712 18:57:23.723971 140178538243968 basic_session_run_hooks.py:260] loss = 1.8321568, step = 401 (101.087 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.987228\n",
            "I0712 18:59:05.016077 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.987228\n",
            "INFO:tensorflow:loss = 1.3575078, step = 501 (101.294 sec)\n",
            "I0712 18:59:05.017483 140178538243968 basic_session_run_hooks.py:260] loss = 1.3575078, step = 501 (101.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.974444\n",
            "I0712 19:00:47.638703 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.974444\n",
            "INFO:tensorflow:loss = 1.1856487, step = 601 (102.623 sec)\n",
            "I0712 19:00:47.640199 140178538243968 basic_session_run_hooks.py:260] loss = 1.1856487, step = 601 (102.623 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 632 into training/model.ckpt.\n",
            "I0712 19:01:18.559542 140178538243968 basic_session_run_hooks.py:606] Saving checkpoints for 632 into training/model.ckpt.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7da7b8b650>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0712 19:01:20.226982 140178538243968 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7da7b8b650>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7da8482440> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0712 19:01:20.494205 140178538243968 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7da8482440> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0712 19:01:21.296797 140178538243968 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:01:24.895925 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:01:24.942510 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:01:24.986607 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:01:25.030884 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:01:25.076637 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:01:25.119627 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/eval_util.py:830: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0712 19:01:26.273296 140178538243968 deprecation.py:323] From /content/models/research/object_detection/eval_util.py:830: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0712 19:01:26.559930 140178538243968 deprecation.py:323] From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0712 19:01:27.290032 140178538243968 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2022-07-12T19:01:27Z\n",
            "I0712 19:01:27.314019 140178538243968 evaluation.py:255] Starting evaluation at 2022-07-12T19:01:27Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0712 19:01:27.916183 140178538243968 monitored_session.py:240] Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-632\n",
            "I0712 19:01:27.918770 140178538243968 saver.py:1284] Restoring parameters from training/model.ckpt-632\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0712 19:01:30.794741 140178538243968 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0712 19:01:31.042938 140178538243968 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 9 images.\n",
            "I0712 19:01:39.276437 140173271910144 coco_evaluation.py:237] Performing evaluation on 9 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0712 19:01:39.276960 140173271910144 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0712 19:01:39.278129 140173271910144 coco_tools.py:138] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.04s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.105\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.342\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.038\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.105\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.130\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.188\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.207\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.207\n",
            "INFO:tensorflow:Finished evaluation at 2022-07-12-19:01:40\n",
            "I0712 19:01:40.974619 140178538243968 evaluation.py:275] Finished evaluation at 2022-07-12-19:01:40\n",
            "INFO:tensorflow:Saving dict for global step 632: DetectionBoxes_Precision/mAP = 0.10529115, DetectionBoxes_Precision/mAP (large) = 0.10530117, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.34167218, DetectionBoxes_Precision/mAP@.75IOU = 0.038366336, DetectionBoxes_Recall/AR@1 = 0.13, DetectionBoxes_Recall/AR@10 = 0.18833333, DetectionBoxes_Recall/AR@100 = 0.20666666, DetectionBoxes_Recall/AR@100 (large) = 0.20666666, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 3.2692444, Loss/localization_loss = 1.0012695, Loss/regularization_loss = 0.24798568, Loss/total_loss = 4.5184994, global_step = 632, learning_rate = 0.004, loss = 4.5184994\n",
            "I0712 19:01:40.974979 140178538243968 estimator.py:2049] Saving dict for global step 632: DetectionBoxes_Precision/mAP = 0.10529115, DetectionBoxes_Precision/mAP (large) = 0.10530117, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.34167218, DetectionBoxes_Precision/mAP@.75IOU = 0.038366336, DetectionBoxes_Recall/AR@1 = 0.13, DetectionBoxes_Recall/AR@10 = 0.18833333, DetectionBoxes_Recall/AR@100 = 0.20666666, DetectionBoxes_Recall/AR@100 (large) = 0.20666666, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 3.2692444, Loss/localization_loss = 1.0012695, Loss/regularization_loss = 0.24798568, Loss/total_loss = 4.5184994, global_step = 632, learning_rate = 0.004, loss = 4.5184994\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 632: training/model.ckpt-632\n",
            "I0712 19:01:41.874354 140178538243968 estimator.py:2109] Saving 'checkpoint_path' summary for global step 632: training/model.ckpt-632\n",
            "INFO:tensorflow:global_step/sec: 0.810526\n",
            "I0712 19:02:51.015379 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.810526\n",
            "INFO:tensorflow:loss = 1.0539012, step = 701 (123.377 sec)\n",
            "I0712 19:02:51.016839 140178538243968 basic_session_run_hooks.py:260] loss = 1.0539012, step = 701 (123.377 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.0032\n",
            "I0712 19:04:30.696613 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 1.0032\n",
            "INFO:tensorflow:loss = 1.1226653, step = 801 (99.681 sec)\n",
            "I0712 19:04:30.697999 140178538243968 basic_session_run_hooks.py:260] loss = 1.1226653, step = 801 (99.681 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.00936\n",
            "I0712 19:06:09.769745 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 1.00936\n",
            "INFO:tensorflow:loss = 1.8196757, step = 901 (99.073 sec)\n",
            "I0712 19:06:09.771139 140178538243968 basic_session_run_hooks.py:260] loss = 1.8196757, step = 901 (99.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.00676\n",
            "I0712 19:07:49.098600 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 1.00676\n",
            "INFO:tensorflow:loss = 1.2351782, step = 1001 (99.329 sec)\n",
            "I0712 19:07:49.099765 140178538243968 basic_session_run_hooks.py:260] loss = 1.2351782, step = 1001 (99.329 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.01599\n",
            "I0712 19:09:27.524853 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 1.01599\n",
            "INFO:tensorflow:loss = 1.0084568, step = 1101 (98.427 sec)\n",
            "I0712 19:09:27.526395 140178538243968 basic_session_run_hooks.py:260] loss = 1.0084568, step = 1101 (98.427 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.01356\n",
            "I0712 19:11:06.186630 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 1.01356\n",
            "INFO:tensorflow:loss = 1.3021376, step = 1201 (98.662 sec)\n",
            "I0712 19:11:06.187938 140178538243968 basic_session_run_hooks.py:260] loss = 1.3021376, step = 1201 (98.662 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1214 into training/model.ckpt.\n",
            "I0712 19:11:19.357301 140178538243968 basic_session_run_hooks.py:606] Saving checkpoints for 1214 into training/model.ckpt.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7c8e1d69d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0712 19:11:20.952875 140178538243968 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7c8e1d69d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7c88868830> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0712 19:11:21.235928 140178538243968 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7c88868830> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0712 19:11:22.004031 140178538243968 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:11:25.197218 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:11:25.244404 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:11:25.291285 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:11:25.340146 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:11:25.388714 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:11:25.435463 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0712 19:11:28.304367 140178538243968 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2022-07-12T19:11:28Z\n",
            "I0712 19:11:28.328001 140178538243968 evaluation.py:255] Starting evaluation at 2022-07-12T19:11:28Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0712 19:11:28.915783 140178538243968 monitored_session.py:240] Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-1214\n",
            "I0712 19:11:28.917969 140178538243968 saver.py:1284] Restoring parameters from training/model.ckpt-1214\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0712 19:11:31.836662 140178538243968 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0712 19:11:32.085839 140178538243968 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 9 images.\n",
            "I0712 19:11:40.293619 140173095663360 coco_evaluation.py:237] Performing evaluation on 9 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0712 19:11:40.294368 140173095663360 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0712 19:11:40.295393 140173095663360 coco_tools.py:138] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.04s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.644\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.303\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.158\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.473\n",
            "INFO:tensorflow:Finished evaluation at 2022-07-12-19:11:41\n",
            "I0712 19:11:41.990572 140178538243968 evaluation.py:275] Finished evaluation at 2022-07-12-19:11:41\n",
            "INFO:tensorflow:Saving dict for global step 1214: DetectionBoxes_Precision/mAP = 0.30288902, DetectionBoxes_Precision/mAP (large) = 0.30290127, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.6437467, DetectionBoxes_Precision/mAP@.75IOU = 0.22554168, DetectionBoxes_Recall/AR@1 = 0.1575, DetectionBoxes_Recall/AR@10 = 0.42833334, DetectionBoxes_Recall/AR@100 = 0.47333333, DetectionBoxes_Recall/AR@100 (large) = 0.47333333, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 1.8900828, Loss/localization_loss = 0.6493453, Loss/regularization_loss = 0.24780127, Loss/total_loss = 2.7872298, global_step = 1214, learning_rate = 0.004, loss = 2.7872298\n",
            "I0712 19:11:41.991011 140178538243968 estimator.py:2049] Saving dict for global step 1214: DetectionBoxes_Precision/mAP = 0.30288902, DetectionBoxes_Precision/mAP (large) = 0.30290127, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.6437467, DetectionBoxes_Precision/mAP@.75IOU = 0.22554168, DetectionBoxes_Recall/AR@1 = 0.1575, DetectionBoxes_Recall/AR@10 = 0.42833334, DetectionBoxes_Recall/AR@100 = 0.47333333, DetectionBoxes_Recall/AR@100 (large) = 0.47333333, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 1.8900828, Loss/localization_loss = 0.6493453, Loss/regularization_loss = 0.24780127, Loss/total_loss = 2.7872298, global_step = 1214, learning_rate = 0.004, loss = 2.7872298\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1214: training/model.ckpt-1214\n",
            "I0712 19:11:42.005866 140178538243968 estimator.py:2109] Saving 'checkpoint_path' summary for global step 1214: training/model.ckpt-1214\n",
            "INFO:tensorflow:global_step/sec: 0.824867\n",
            "I0712 19:13:07.418289 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.824867\n",
            "INFO:tensorflow:loss = 0.9408488, step = 1301 (121.232 sec)\n",
            "I0712 19:13:07.419769 140178538243968 basic_session_run_hooks.py:260] loss = 0.9408488, step = 1301 (121.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.02112\n",
            "I0712 19:14:45.349961 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 1.02112\n",
            "INFO:tensorflow:loss = 1.3588707, step = 1401 (97.931 sec)\n",
            "I0712 19:14:45.351202 140178538243968 basic_session_run_hooks.py:260] loss = 1.3588707, step = 1401 (97.931 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.02182\n",
            "I0712 19:16:23.214770 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 1.02182\n",
            "INFO:tensorflow:loss = 0.9826941, step = 1501 (97.865 sec)\n",
            "I0712 19:16:23.216246 140178538243968 basic_session_run_hooks.py:260] loss = 0.9826941, step = 1501 (97.865 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.00464\n",
            "I0712 19:18:02.752472 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 1.00464\n",
            "INFO:tensorflow:loss = 1.2364583, step = 1601 (99.538 sec)\n",
            "I0712 19:18:02.753751 140178538243968 basic_session_run_hooks.py:260] loss = 1.2364583, step = 1601 (99.538 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.00175\n",
            "I0712 19:19:42.577951 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 1.00175\n",
            "INFO:tensorflow:loss = 1.2148176, step = 1701 (99.826 sec)\n",
            "I0712 19:19:42.579407 140178538243968 basic_session_run_hooks.py:260] loss = 1.2148176, step = 1701 (99.826 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1799 into training/model.ckpt.\n",
            "I0712 19:21:20.305144 140178538243968 basic_session_run_hooks.py:606] Saving checkpoints for 1799 into training/model.ckpt.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7c8b9bdb50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0712 19:21:21.918404 140178538243968 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7c8b9bdb50>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7c8d6b0710> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0712 19:21:22.176749 140178538243968 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7c8d6b0710> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0712 19:21:23.014503 140178538243968 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:21:26.230028 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:21:26.276014 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:21:26.321280 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:21:26.367677 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:21:26.413669 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:21:26.459790 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0712 19:21:28.734343 140178538243968 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2022-07-12T19:21:28Z\n",
            "I0712 19:21:28.759001 140178538243968 evaluation.py:255] Starting evaluation at 2022-07-12T19:21:28Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0712 19:21:29.375360 140178538243968 monitored_session.py:240] Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-1799\n",
            "I0712 19:21:29.377675 140178538243968 saver.py:1284] Restoring parameters from training/model.ckpt-1799\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0712 19:21:32.211512 140178538243968 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0712 19:21:32.451201 140178538243968 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 9 images.\n",
            "I0712 19:21:40.697517 140173322266368 coco_evaluation.py:237] Performing evaluation on 9 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0712 19:21:40.698013 140173322266368 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0712 19:21:40.699653 140173322266368 coco_tools.py:138] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.05s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.593\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.345\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.329\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.264\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.426\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.426\n",
            "INFO:tensorflow:Finished evaluation at 2022-07-12-19:21:42\n",
            "I0712 19:21:42.448013 140178538243968 evaluation.py:275] Finished evaluation at 2022-07-12-19:21:42\n",
            "INFO:tensorflow:Saving dict for global step 1799: DetectionBoxes_Precision/mAP = 0.32906324, DetectionBoxes_Precision/mAP (large) = 0.32906324, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.5931558, DetectionBoxes_Precision/mAP@.75IOU = 0.34503374, DetectionBoxes_Recall/AR@1 = 0.26416665, DetectionBoxes_Recall/AR@10 = 0.38166666, DetectionBoxes_Recall/AR@100 = 0.42583334, DetectionBoxes_Recall/AR@100 (large) = 0.42583334, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 0.9212641, Loss/localization_loss = 0.32410213, Loss/regularization_loss = 0.24757531, Loss/total_loss = 1.4929415, global_step = 1799, learning_rate = 0.004, loss = 1.4929415\n",
            "I0712 19:21:42.448412 140178538243968 estimator.py:2049] Saving dict for global step 1799: DetectionBoxes_Precision/mAP = 0.32906324, DetectionBoxes_Precision/mAP (large) = 0.32906324, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.5931558, DetectionBoxes_Precision/mAP@.75IOU = 0.34503374, DetectionBoxes_Recall/AR@1 = 0.26416665, DetectionBoxes_Recall/AR@10 = 0.38166666, DetectionBoxes_Recall/AR@100 = 0.42583334, DetectionBoxes_Recall/AR@100 (large) = 0.42583334, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 0.9212641, Loss/localization_loss = 0.32410213, Loss/regularization_loss = 0.24757531, Loss/total_loss = 1.4929415, global_step = 1799, learning_rate = 0.004, loss = 1.4929415\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1799: training/model.ckpt-1799\n",
            "I0712 19:21:42.463455 140178538243968 estimator.py:2109] Saving 'checkpoint_path' summary for global step 1799: training/model.ckpt-1799\n",
            "INFO:tensorflow:global_step/sec: 0.819285\n",
            "I0712 19:21:44.635593 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.819285\n",
            "INFO:tensorflow:loss = 0.8977951, step = 1801 (122.058 sec)\n",
            "I0712 19:21:44.636962 140178538243968 basic_session_run_hooks.py:260] loss = 0.8977951, step = 1801 (122.058 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.979002\n",
            "I0712 19:23:26.780472 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.979002\n",
            "INFO:tensorflow:loss = 1.5688392, step = 1901 (102.145 sec)\n",
            "I0712 19:23:26.781812 140178538243968 basic_session_run_hooks.py:260] loss = 1.5688392, step = 1901 (102.145 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.990541\n",
            "I0712 19:25:07.735301 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.990541\n",
            "INFO:tensorflow:loss = 0.8269398, step = 2001 (100.955 sec)\n",
            "I0712 19:25:07.736779 140178538243968 basic_session_run_hooks.py:260] loss = 0.8269398, step = 2001 (100.955 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.996977\n",
            "I0712 19:26:48.038511 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.996977\n",
            "INFO:tensorflow:loss = 0.86039084, step = 2101 (100.303 sec)\n",
            "I0712 19:26:48.039833 140178538243968 basic_session_run_hooks.py:260] loss = 0.86039084, step = 2101 (100.303 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.988134\n",
            "I0712 19:28:29.239359 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.988134\n",
            "INFO:tensorflow:loss = 1.0331092, step = 2201 (101.201 sec)\n",
            "I0712 19:28:29.240957 140178538243968 basic_session_run_hooks.py:260] loss = 1.0331092, step = 2201 (101.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.976689\n",
            "I0712 19:30:11.626016 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.976689\n",
            "INFO:tensorflow:loss = 0.72221833, step = 2301 (102.386 sec)\n",
            "I0712 19:30:11.627192 140178538243968 basic_session_run_hooks.py:260] loss = 0.72221833, step = 2301 (102.386 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2369 into training/model.ckpt.\n",
            "I0712 19:31:20.375154 140178538243968 basic_session_run_hooks.py:606] Saving checkpoints for 2369 into training/model.ckpt.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7c8bbacbd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0712 19:31:21.955061 140178538243968 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7c8bbacbd0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7c8bb8f170> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0712 19:31:22.240568 140178538243968 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7c8bb8f170> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0712 19:31:23.711388 140178538243968 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:31:26.875870 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:31:26.924667 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:31:26.968863 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:31:27.016111 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:31:27.059747 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:31:27.103702 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0712 19:31:29.324112 140178538243968 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2022-07-12T19:31:29Z\n",
            "I0712 19:31:29.349259 140178538243968 evaluation.py:255] Starting evaluation at 2022-07-12T19:31:29Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0712 19:31:29.981348 140178538243968 monitored_session.py:240] Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-2369\n",
            "I0712 19:31:29.984013 140178538243968 saver.py:1284] Restoring parameters from training/model.ckpt-2369\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0712 19:31:32.901592 140178538243968 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0712 19:31:33.150656 140178538243968 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 9 images.\n",
            "I0712 19:31:41.582239 140173078877952 coco_evaluation.py:237] Performing evaluation on 9 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0712 19:31:41.582735 140173078877952 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0712 19:31:41.583766 140173078877952 coco_tools.py:138] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.05s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.03s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.536\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.891\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.587\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.411\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.598\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.598\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.598\n",
            "INFO:tensorflow:Finished evaluation at 2022-07-12-19:31:43\n",
            "I0712 19:31:43.321596 140178538243968 evaluation.py:275] Finished evaluation at 2022-07-12-19:31:43\n",
            "INFO:tensorflow:Saving dict for global step 2369: DetectionBoxes_Precision/mAP = 0.5359331, DetectionBoxes_Precision/mAP (large) = 0.5359331, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.8910231, DetectionBoxes_Precision/mAP@.75IOU = 0.58679867, DetectionBoxes_Recall/AR@1 = 0.41083333, DetectionBoxes_Recall/AR@10 = 0.5975, DetectionBoxes_Recall/AR@100 = 0.5975, DetectionBoxes_Recall/AR@100 (large) = 0.5975, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 0.656736, Loss/localization_loss = 0.19380672, Loss/regularization_loss = 0.24732974, Loss/total_loss = 1.0978724, global_step = 2369, learning_rate = 0.004, loss = 1.0978724\n",
            "I0712 19:31:43.321947 140178538243968 estimator.py:2049] Saving dict for global step 2369: DetectionBoxes_Precision/mAP = 0.5359331, DetectionBoxes_Precision/mAP (large) = 0.5359331, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.8910231, DetectionBoxes_Precision/mAP@.75IOU = 0.58679867, DetectionBoxes_Recall/AR@1 = 0.41083333, DetectionBoxes_Recall/AR@10 = 0.5975, DetectionBoxes_Recall/AR@100 = 0.5975, DetectionBoxes_Recall/AR@100 (large) = 0.5975, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 0.656736, Loss/localization_loss = 0.19380672, Loss/regularization_loss = 0.24732974, Loss/total_loss = 1.0978724, global_step = 2369, learning_rate = 0.004, loss = 1.0978724\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2369: training/model.ckpt-2369\n",
            "I0712 19:31:43.337015 140178538243968 estimator.py:2109] Saving 'checkpoint_path' summary for global step 2369: training/model.ckpt-2369\n",
            "INFO:tensorflow:global_step/sec: 0.805504\n",
            "I0712 19:32:15.771875 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.805504\n",
            "INFO:tensorflow:loss = 0.99423444, step = 2401 (124.146 sec)\n",
            "I0712 19:32:15.773145 140178538243968 basic_session_run_hooks.py:260] loss = 0.99423444, step = 2401 (124.146 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.965927\n",
            "I0712 19:33:59.299399 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.965927\n",
            "INFO:tensorflow:loss = 0.83613473, step = 2501 (103.528 sec)\n",
            "I0712 19:33:59.301225 140178538243968 basic_session_run_hooks.py:260] loss = 0.83613473, step = 2501 (103.528 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.985665\n",
            "I0712 19:35:40.753733 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.985665\n",
            "INFO:tensorflow:loss = 0.69721544, step = 2601 (101.454 sec)\n",
            "I0712 19:35:40.755059 140178538243968 basic_session_run_hooks.py:260] loss = 0.69721544, step = 2601 (101.454 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.983782\n",
            "I0712 19:37:22.402253 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.983782\n",
            "INFO:tensorflow:loss = 0.8675431, step = 2701 (101.649 sec)\n",
            "I0712 19:37:22.403705 140178538243968 basic_session_run_hooks.py:260] loss = 0.8675431, step = 2701 (101.649 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.976144\n",
            "I0712 19:39:04.846132 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.976144\n",
            "INFO:tensorflow:loss = 0.663718, step = 2801 (102.444 sec)\n",
            "I0712 19:39:04.847238 140178538243968 basic_session_run_hooks.py:260] loss = 0.663718, step = 2801 (102.444 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.983027\n",
            "I0712 19:40:46.572727 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.983027\n",
            "INFO:tensorflow:loss = 0.6751834, step = 2901 (101.727 sec)\n",
            "I0712 19:40:46.574022 140178538243968 basic_session_run_hooks.py:260] loss = 0.6751834, step = 2901 (101.727 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2934 into training/model.ckpt.\n",
            "I0712 19:41:20.848145 140178538243968 basic_session_run_hooks.py:606] Saving checkpoints for 2934 into training/model.ckpt.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "W0712 19:41:21.062343 140178538243968 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7c8ba90b10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0712 19:41:22.468578 140178538243968 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7c8ba90b10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7c8ba860e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "W0712 19:41:22.742456 140178538243968 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7c8ba860e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0712 19:41:23.495050 140178538243968 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:41:26.741056 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:41:26.788413 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:41:26.833896 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:41:26.879792 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:41:26.927145 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0712 19:41:27.483658 140178538243968 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0712 19:41:29.762326 140178538243968 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2022-07-12T19:41:29Z\n",
            "I0712 19:41:29.788058 140178538243968 evaluation.py:255] Starting evaluation at 2022-07-12T19:41:29Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0712 19:41:30.427586 140178538243968 monitored_session.py:240] Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-2934\n",
            "I0712 19:41:30.429811 140178538243968 saver.py:1284] Restoring parameters from training/model.ckpt-2934\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0712 19:41:33.333697 140178538243968 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0712 19:41:33.585660 140178538243968 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 9 images.\n",
            "I0712 19:41:42.047734 140173062092544 coco_evaluation.py:237] Performing evaluation on 9 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0712 19:41:42.048177 140173062092544 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0712 19:41:42.049162 140173062092544 coco_tools.py:138] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.05s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.03s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.583\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.993\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.523\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.583\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.413\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.603\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.603\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.603\n",
            "INFO:tensorflow:Finished evaluation at 2022-07-12-19:41:43\n",
            "I0712 19:41:43.740766 140178538243968 evaluation.py:275] Finished evaluation at 2022-07-12-19:41:43\n",
            "INFO:tensorflow:Saving dict for global step 2934: DetectionBoxes_Precision/mAP = 0.58315784, DetectionBoxes_Precision/mAP (large) = 0.58315784, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.9929793, DetectionBoxes_Precision/mAP@.75IOU = 0.52347434, DetectionBoxes_Recall/AR@1 = 0.41333333, DetectionBoxes_Recall/AR@10 = 0.60333335, DetectionBoxes_Recall/AR@100 = 0.60333335, DetectionBoxes_Recall/AR@100 (large) = 0.60333335, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 0.4908044, Loss/localization_loss = 0.12236973, Loss/regularization_loss = 0.24707253, Loss/total_loss = 0.86024666, global_step = 2934, learning_rate = 0.004, loss = 0.86024666\n",
            "I0712 19:41:43.741097 140178538243968 estimator.py:2049] Saving dict for global step 2934: DetectionBoxes_Precision/mAP = 0.58315784, DetectionBoxes_Precision/mAP (large) = 0.58315784, DetectionBoxes_Precision/mAP (medium) = -1.0, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.9929793, DetectionBoxes_Precision/mAP@.75IOU = 0.52347434, DetectionBoxes_Recall/AR@1 = 0.41333333, DetectionBoxes_Recall/AR@10 = 0.60333335, DetectionBoxes_Recall/AR@100 = 0.60333335, DetectionBoxes_Recall/AR@100 (large) = 0.60333335, DetectionBoxes_Recall/AR@100 (medium) = -1.0, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/classification_loss = 0.4908044, Loss/localization_loss = 0.12236973, Loss/regularization_loss = 0.24707253, Loss/total_loss = 0.86024666, global_step = 2934, learning_rate = 0.004, loss = 0.86024666\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2934: training/model.ckpt-2934\n",
            "I0712 19:41:43.755953 140178538243968 estimator.py:2109] Saving 'checkpoint_path' summary for global step 2934: training/model.ckpt-2934\n",
            "INFO:tensorflow:global_step/sec: 0.799016\n",
            "I0712 19:42:51.726694 140178538243968 basic_session_run_hooks.py:692] global_step/sec: 0.799016\n",
            "INFO:tensorflow:loss = 1.0303546, step = 3001 (125.154 sec)\n",
            "I0712 19:42:51.728411 140178538243968 basic_session_run_hooks.py:260] loss = 1.0303546, step = 3001 (125.154 sec)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/models/research/object_detection/model_main.py\", line 114, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 312, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 258, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/content/models/research/object_detection/model_main.py\", line 110, in main\n",
            "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\n",
            "    return executor.run()\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/training.py\", line 613, in run\n",
            "    return self.run_local()\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\n",
            "    saving_listeners=saving_listeners)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\n",
            "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\n",
            "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\n",
            "    saving_listeners)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_estimator/python/estimator/estimator.py\", line 1494, in _train_with_estimator_spec\n",
            "    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/training/monitored_session.py\", line 754, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/training/monitored_session.py\", line 1259, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/training/monitored_session.py\", line 1345, in run\n",
            "    return self._sess.run(*args, **kwargs)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/training/monitored_session.py\", line 1418, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/training/monitored_session.py\", line 1176, in run\n",
            "    return self._sess.run(*args, **kwargs)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "model_dir = 'training/'\n",
        "# Optionally remove content in output model directory for a fresh start.\n",
        "# !rm -rf {model_dir}\n",
        "# os.makedirs(model_dir, exist_ok=True)\n",
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --num_eval_steps={num_eval_steps}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baYhLBYH9WVI",
        "outputId": "2fae2559-544c-4760-8944-f858e5e4463e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint\n",
            "eval_0\n",
            "events.out.tfevents.1657651860.cdb5970f856a\n",
            "graph.pbtxt\n",
            "model.ckpt-1214.data-00000-of-00001\n",
            "model.ckpt-1214.index\n",
            "model.ckpt-1214.meta\n",
            "model.ckpt-1799.data-00000-of-00001\n",
            "model.ckpt-1799.index\n",
            "model.ckpt-1799.meta\n",
            "model.ckpt-2369.data-00000-of-00001\n",
            "model.ckpt-2369.index\n",
            "model.ckpt-2369.meta\n",
            "model.ckpt-2934.data-00000-of-00001\n",
            "model.ckpt-2934.index\n",
            "model.ckpt-2934.meta\n",
            "model.ckpt-632.data-00000-of-00001\n",
            "model.ckpt-632.index\n",
            "model.ckpt-632.meta\n"
          ]
        }
      ],
      "source": [
        "#model dir check for the trained model\n",
        "!ls {model_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GxrPWUH9XZq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "output_directory = './fine_tuned_model'\n",
        "# output_directory = '/content/gdrive/My\\ Drive/data/'\n",
        "\n",
        "lst = os.listdir(model_dir)\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "print(last_model_path)\n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory={output_directory} \\\n",
        "    --trained_checkpoint_prefix={last_model_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHLl_3Vm9vRZ",
        "outputId": "24aec3ee-62fe-401d-c61b-5a3a850e17a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint\t\t\tmodel.ckpt.index  saved_model\n",
            "frozen_inference_graph.pb\tmodel.ckpt.meta\n",
            "model.ckpt.data-00000-of-00001\tpipeline.config\n"
          ]
        }
      ],
      "source": [
        "#export directory check\n",
        "!ls {output_directory}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyFFUIav9zUg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")\n",
        "assert os.path.isfile(pb_fname), '`{}` not exist'.format(pb_fname)\n",
        "# !ls -alh {pb_fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj3z4fHN91wh"
      },
      "outputs": [],
      "source": [
        "####  testing\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = pb_fname\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_TEST_IMAGES_DIR =  \"/content/test\"\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.*\"))\n",
        "assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMwDnVjj-Afa",
        "outputId": "a7c16682-0db2-4c9a-c5e7-b3a708dbb10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/object_detection\n"
          ]
        }
      ],
      "source": [
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaZeFR78yApt",
        "outputId": "32e3dac8-1f7a-4519-cf51-41cea0ee75d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test/cw (9).jpg\n",
            "/content/test/cot (1).jpg\n",
            "/content/test/rw (7).jpg\n",
            "/content/test/ec (7).jpg\n",
            "/content/test/rw (9).jpg\n",
            "/content/test/rw (1).jpg\n",
            "/content/test/cot (7).jpg\n",
            "/content/test/cot (5).jpg\n"
          ]
        }
      ],
      "source": [
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_VAL_IMAGES_DIR =  \"/content/test\"\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "VAL_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_VAL_IMAGES_DIR, \"*.jpg\"))\n",
        "assert len(VAL_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_VAL_IMAGES_DIR)\n",
        "\n",
        "for image_path in VAL_IMAGE_PATHS:\n",
        "  image = Image.open(image_path)\n",
        "  print(image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  res = vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=16)\n",
        "  plt.figure(figsize=IMAGE_SIZE)\n",
        "  plt.imshow(res)\n",
        "  plt.show()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDzvLbhIFpAa",
        "outputId": "70808e5d-5abd-49a5-e638-24a65d7b8802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test/cw (9).jpg\n",
            "/content/test/cot (1).jpg\n",
            "/content/test/rw (7).jpg\n",
            "/content/test/ec (7).jpg\n",
            "/content/test/rw (9).jpg\n",
            "/content/test/rw (1).jpg\n",
            "/content/test/cot (7).jpg\n",
            "/content/test/cot (5).jpg\n"
          ]
        }
      ],
      "source": [
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_VAL_IMAGES_DIR =  \"/content/test\"\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "VAL_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_VAL_IMAGES_DIR, \"*.jpg\"))\n",
        "assert len(VAL_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_VAL_IMAGES_DIR)\n",
        "i=0\n",
        "for image_path in VAL_IMAGE_PATHS:\n",
        "  image = Image.open(image_path)\n",
        "  print(image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  res = vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=16)\n",
        "  plt.figure(figsize=IMAGE_SIZE)\n",
        "  plt.imshow(res)\n",
        "  plt.show()\n",
        "  i+=1\n",
        "  plt.imsave('/content/gdrive/MyDrive/New_Obj_det_Datset/results/a'+str(i)+'res.jpeg', res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xopkkRwvLwUU",
        "outputId": "333de307-f628-467f-90ae-709763fcae40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/validation/rw (6).JPG\n",
            "/content/validation/mg (3).JPG\n",
            "/content/validation/mg (10).JPG\n",
            "/content/validation/cw (12).JPG\n",
            "/content/validation/ec (6).JPG\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/validation/ec (4).JPG\n"
          ]
        }
      ],
      "source": [
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_VAL_IMAGES_DIR =  \"/content/validation\"\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "VAL_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_VAL_IMAGES_DIR, \"*.JPG\"))\n",
        "assert len(VAL_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_VAL_IMAGES_DIR)\n",
        "for image_path in VAL_IMAGE_PATHS:\n",
        "  image = Image.open(image_path)\n",
        "  print(image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  res = vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=16)\n",
        "  plt.figure(figsize=IMAGE_SIZE)\n",
        "  plt.imshow(res)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAso06qlzCSM"
      },
      "outputs": [],
      "source": [
        "plt.imsave('/content/gdrive/MyDrive/New_Obj_det_Datset/results/b'+str(i)+'.jpeg', res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L4MNhO8ykDh",
        "outputId": "568b9b8e-838a-480e-9187-35bf99ef4272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/validation/cot (4).jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/validation/cw (8).jpg\n",
            "/content/validation/cot (6).jpg\n"
          ]
        }
      ],
      "source": [
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_VAL_IMAGES_DIR =  \"/content/validation\"\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "VAL_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_VAL_IMAGES_DIR, \"*.jpg\"))\n",
        "assert len(VAL_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_VAL_IMAGES_DIR)\n",
        "for image_path in VAL_IMAGE_PATHS:\n",
        "  image = Image.open(image_path)\n",
        "  print(image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  res = vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=16)\n",
        "  plt.figure(figsize=IMAGE_SIZE)\n",
        "  plt.imshow(res)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKIqEKDxPj8o",
        "outputId": "22b8cce9-7285-490e-aab5-16af22f05779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/validation/cot (4).jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/validation/cw (8).jpg\n",
            "/content/validation/cot (6).jpg\n"
          ]
        }
      ],
      "source": [
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_VAL_IMAGES_DIR =  \"/content/validation\"\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "VAL_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_VAL_IMAGES_DIR, \"*.jpg\"))\n",
        "assert len(VAL_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_VAL_IMAGES_DIR)\n",
        "i=0\n",
        "for image_path in VAL_IMAGE_PATHS:\n",
        "  image = Image.open(image_path)\n",
        "  print(image_path)\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = load_image_into_numpy_array(image)\n",
        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "  # Visualization of the results of a detection.\n",
        "  res = vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=16)\n",
        "  plt.figure(figsize=IMAGE_SIZE)\n",
        "  plt.imshow(res)\n",
        "  plt.show()\n",
        "  i+=1\n",
        "  plt.imsave('/content/gdrive/MyDrive/New_Obj_det_Datset/results/c'+str(i)+'.jpeg', res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCQ9S15MPmvv",
        "outputId": "7e7133c8-5fca-46c4-f453-51fba4dcd58c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/obj_det_img/test2.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        }
      ],
      "source": [
        "image_path = '/content/gdrive/MyDrive/obj_det_img/test2.jpg'\n",
        "image = Image.open(image_path)\n",
        "print(image_path)\n",
        "# the array based representation of the image will be used later in order to prepare the\n",
        "# result image with boxes and labels on it.\n",
        "image_np = load_image_into_numpy_array(image)\n",
        "# Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "# Actual detection.\n",
        "output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "# Visualization of the results of a detection.\n",
        "res = vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np,\n",
        "    output_dict['detection_boxes'],\n",
        "    output_dict['detection_classes'],\n",
        "    output_dict['detection_scores'],\n",
        "    category_index,\n",
        "    instance_masks=output_dict.get('detection_masks'),\n",
        "    use_normalized_coordinates=True,\n",
        "    line_thickness=16)\n",
        "plt.figure(figsize=IMAGE_SIZE)\n",
        "plt.imshow(res)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn_8-h3kyayN"
      },
      "outputs": [],
      "source": [
        "plt.imsave('/content/gdrive/MyDrive/New_Obj_det_Datset/results/test_res_.jpeg', res)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "object detection ssd v3 new_dataset ( 10000 steps) .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}